{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenova13q/paw_finder/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_clip_ipynb%22%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FoLxEynVhUkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c98ae31-b28a-4a58-a454-fcad28c63f09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jL1tE-aFduBX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch torchvision\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install pillow\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f32BO2Cxdu9m"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f-vHZbSIdmce"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "import clip\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torch.optim import AdamW\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T8UK_L8sgQPb"
      },
      "outputs": [],
      "source": [
        "images_path = \"/content/drive/MyDrive/Colab Notebooks/parses/cat_images\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V9py_8Lmdje7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Функция для создания описания\n",
        "def create_description(pet_id, details):\n",
        "    description = \"\"#f\"**Cat ID:** {pet_id}\\n\"\n",
        "\n",
        "    age = details.get('age')\n",
        "    primary_breed = details.get('primary_breed')\n",
        "    species = details.get('species', 'Кошка')\n",
        "    color = details.get('color')\n",
        "    hair_length = details.get('hair_length')\n",
        "    sex = details.get('sex')\n",
        "    purebred = details.get('purebred')\n",
        "\n",
        "    if age:\n",
        "        description += f\"{age} \"\n",
        "    if primary_breed:\n",
        "        description += f\"{primary_breed} \"\n",
        "    else:\n",
        "        description += f\"{species.lower()} \"\n",
        "    if color:\n",
        "        description += f\"cat with {color.lower()} color. This is a \"\n",
        "    if purebred is not None:\n",
        "        description += f\"{'purebred ' if purebred else ''}\"\n",
        "    if sex:\n",
        "        sex_string = \"female cat \" if sex == 'Female' else \"male cat \"\n",
        "        description += sex_string\n",
        "    if hair_length:\n",
        "        description += f\"with {hair_length.lower()} hair.\"\n",
        "\n",
        "    return description.strip() + \"\\n\"\n",
        "\n",
        "# Чтение данных из файла\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/parses/detailed_pets_data.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Создание описаний для каждого питомца\n",
        "descriptions = {}\n",
        "for pet_id, pet_data in data.items():\n",
        "    formatted_pet_id = str(pet_id).zfill(6)\n",
        "    details = pet_data.get('details', {})\n",
        "    description = create_description(formatted_pet_id, details)\n",
        "    descriptions[formatted_pet_id] = description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBE3Ss5lggDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ed8893-094f-4fe4-850b-8a6bfd734478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Young Siamese cat with white (mostly) color. This is a purebred female cat with short hair.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(descriptions['000001'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(descriptions['000002'])"
      ],
      "metadata": {
        "id": "pwjUT6yTi4S0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adcb9e4-c498-4e48-80cd-bf02f8ce622d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Young Tabby cat with gray or blue color. This is a female cat with medium hair.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Random seed set as {seed}\")"
      ],
      "metadata": {
        "id": "Mt7b4G-ljUH9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed()"
      ],
      "metadata": {
        "id": "qakuw1XRjXpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394ff010-fb4a-4b0c-e2f1-0512c1b0d5e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set as 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "taqkiU4t76li",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "db57408a-525e-41bf-ac2a-71a425b73696"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_images = [img for img in os.listdir(images_path)]\n",
        "random.shuffle(cat_images)\n",
        "\n",
        "# Создание словаря для преобразования меток\n",
        "unique_labels = list(set([img.split('_')[0] for img in cat_images]))\n",
        "random.shuffle(unique_labels)\n",
        "\n",
        "\n",
        "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
        "\n",
        "# Определение количества кошек для каждого набора\n",
        "num_cats = len(unique_labels)\n",
        "num_train = int(0.7 * num_cats)\n",
        "num_val = int(0.15 * num_cats)\n",
        "num_test = num_cats - num_train - num_val\n",
        "\n",
        "# Разделение кошек на три группы\n",
        "train_labels = unique_labels[:num_train]\n",
        "val_labels = unique_labels[num_train:num_train + num_val]\n",
        "test_labels = unique_labels[num_train + num_val:]\n",
        "\n",
        "# Разделение изображений на три группы\n",
        "train_images = [img for img in cat_images if img.split('_')[0] in train_labels]\n",
        "val_images = [img for img in cat_images if img.split('_')[0] in val_labels]\n",
        "test_images = [img for img in cat_images if img.split('_')[0] in test_labels]\n",
        "\n",
        "# Проверка результатов разделения\n",
        "print(f\"Train images: {len(train_images)}\")\n",
        "print(f\"Validation images: {len(val_images)}\")\n",
        "print(f\"Test images: {len(test_images)}\")\n",
        "print(f\"Number of train cats: {len(train_labels)}\")\n",
        "print(f\"Number of validation cats: {len(val_labels)}\")\n",
        "print(f\"Number of test cats: {len(test_labels)}\")"
      ],
      "metadata": {
        "id": "N9LHbwxAl-7s",
        "outputId": "7a2d783d-1dbe-4ac1-aeed-2bd2d38dcc49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 7167\n",
            "Validation images: 1498\n",
            "Test images: 1566\n",
            "Number of train cats: 2672\n",
            "Number of validation cats: 572\n",
            "Number of test cats: 574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, processor = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "clip.model.convert_weights(model)"
      ],
      "metadata": {
        "id": "EPL4fDR5mXDF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CatDataset(Dataset):\n",
        "    def __init__(self, images, label_to_idx, pet_details):\n",
        "        self.images = images\n",
        "        self.label_to_idx = label_to_idx\n",
        "        self.pet_details = pet_details\n",
        "\n",
        "        self.processed_cache = {}\n",
        "        for img_path in self.images:\n",
        "            self.processed_cache[img_path] = processor(Image.open(f\"{images_path}/{img_path}\"))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = self.processed_cache[img_path]\n",
        "        label = img_path.split('_')[0]\n",
        "        label_idx = self.label_to_idx[label]\n",
        "        details_text = clip.tokenize(self.pet_details[label])\n",
        "\n",
        "        return image, label_idx, details_text"
      ],
      "metadata": {
        "id": "E311j9hxmWva"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epoch_num = 20"
      ],
      "metadata": {
        "id": "h9ZjgeIlxjaH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CatDataset(train_images,  label_to_idx, descriptions)\n",
        "val_dataset = CatDataset(val_images,  label_to_idx, descriptions)\n",
        "test_dataset = CatDataset(test_images,  label_to_idx, descriptions)"
      ],
      "metadata": {
        "id": "4bY0c3JKl4A6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "3R6HvMq1mqjf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ubVC0X5koKJ0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*epoch_num)"
      ],
      "metadata": {
        "id": "-8k-ZZqpx7aG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()"
      ],
      "metadata": {
        "id": "KRjrv4fQJsPQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5-ndJvfjd74i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6689ac31-a36d-448c-d58f-315320c3a5e0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 1/20 - Обучение: 100%|██████████| 224/224 [00:22<00:00,  9.84it/s, loss=2.08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 1 завершена. Потеря на тренировочном наборе: 2.0784170968191966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 1/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 27.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 2.0292137632978724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 2/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.76it/s, loss=1.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 2 завершена. Потеря на тренировочном наборе: 1.4265965053013392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 2/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 2.0573055186170213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 3/20 - Обучение: 100%|██████████| 224/224 [00:21<00:00, 10.60it/s, loss=0.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 3 завершена. Потеря на тренировочном наборе: 0.9002314976283482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 3/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 2.419921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 4/20 - Обучение: 100%|██████████| 224/224 [00:21<00:00, 10.63it/s, loss=0.571]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 4 завершена. Потеря на тренировочном наборе: 0.5680629185267857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 4/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 2.7542802526595747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 5/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.72it/s, loss=0.366]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 5 завершена. Потеря на тренировочном наборе: 0.36462811061314176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 5/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 2.9333028590425534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 6/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.72it/s, loss=0.296]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 6 завершена. Потеря на тренировочном наборе: 0.2945662907191685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 6/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.09711602393617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 7/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.67it/s, loss=0.226]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 7 завершена. Потеря на тренировочном наборе: 0.22489820207868302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 7/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.335210272606383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 8/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.73it/s, loss=0.202]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 8 завершена. Потеря на тренировочном наборе: 0.20132923126220703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 8/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 29.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.3555726396276597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 9/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.74it/s, loss=0.179]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 9 завершена. Потеря на тренировочном наборе: 0.17835283279418945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 9/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.442611369680851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 10/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.70it/s, loss=0.156]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 10 завершена. Потеря на тренировочном наборе: 0.155503511428833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 10/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 29.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.5516539228723403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 11/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.73it/s, loss=0.149]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 11 завершена. Потеря на тренировочном наборе: 0.14847285406930105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 11/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.5738031914893615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 12/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.69it/s, loss=0.133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 12 завершена. Потеря на тренировочном наборе: 0.1325575624193464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 12/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.615400598404255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 13/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.71it/s, loss=0.127]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 13 завершена. Потеря на тренировочном наборе: 0.1268172264099121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 13/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.6434507978723403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 14/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.73it/s, loss=0.124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 14 завершена. Потеря на тренировочном наборе: 0.1230928897857666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 14/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.6891622340425534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 15/20 - Обучение: 100%|██████████| 224/224 [00:21<00:00, 10.65it/s, loss=0.119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 15 завершена. Потеря на тренировочном наборе: 0.11813383443014962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 15/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.7063663563829787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 16/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.77it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 16 завершена. Потеря на тренировочном наборе: 0.11585290942873273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 16/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.7109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 17/20 - Обучение: 100%|██████████| 224/224 [00:20<00:00, 10.68it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 17 завершена. Потеря на тренировочном наборе: 0.113379853112357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 17/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 29.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.7230718085106385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 18/20 - Обучение: 100%|██████████| 224/224 [00:21<00:00, 10.50it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 18 завершена. Потеря на тренировочном наборе: 0.1132295983178275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 18/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 29.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.726354720744681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 19/20 - Обучение: 100%|██████████| 224/224 [00:21<00:00, 10.54it/s, loss=0.113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 19 завершена. Потеря на тренировочном наборе: 0.11295489753995623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 19/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 30.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.728266289893617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 20/20 - Обучение: 100%|██████████| 224/224 [00:21<00:00, 10.53it/s, loss=0.113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 20 завершена. Потеря на тренировочном наборе: 0.11258073363985334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 20/20 - Валидация: 100%|██████████| 47/47 [00:01<00:00, 29.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Потеря на валидационном наборе: 3.7289311835106385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train_model(train_loader, val_loader, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Эпоха {epoch + 1}/{num_epochs} - Обучение\")\n",
        "        for images, labels, details_text in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            details_text = details_text.squeeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits_per_image, logits_per_text = model(images, details_text)\n",
        "\n",
        "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "            total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "            total_loss.backward()\n",
        "            convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            clip.model.convert_weights(model)\n",
        "\n",
        "            running_loss += total_loss.item()\n",
        "            progress_bar.set_postfix(loss=running_loss / (progress_bar.n + 1))\n",
        "        print(f\"Эпоха {epoch + 1} завершена. Потеря на тренировочном наборе: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Оценка на валидационном наборе\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels, details_text in tqdm(val_loader, desc=f\"Эпоха {epoch + 1}/{num_epochs} - Валидация\"):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                details_text = details_text.squeeze(1).to(device)\n",
        "\n",
        "                logits_per_image, logits_per_text = model(images, details_text)\n",
        "\n",
        "                ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "                total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "\n",
        "                val_loss += total_loss.item()\n",
        "            print(f\"Потеря на валидационном наборе: {val_loss / len(val_loader)}\")\n",
        "\n",
        "# Обучение модели\n",
        "train_model(train_loader, val_loader, optimizer, 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kqNtfjdymisM"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, epoch, path=\"model_checkpoint.pth\"):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, path)\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/model_checkpoint_clip_new_1.pth\"\n",
        "save_model(model, optimizer, epoch=20, path=model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка сохраненной модели\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/model_checkpoint_clip_new_1.pth\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "rP_plxmruMDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_list = []\n",
        "train_labels = []\n",
        "\n",
        "for image_name in tqdm(train_images, desc=\"Извлечение признаков для тренировочных данных\"):\n",
        "    image_path = os.path.join(images_path, image_name)\n",
        "    image = Image.open(image_path)\n",
        "    image = processor(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    train_features_list.append(image_features)\n",
        "    train_labels.append(image_name.split('_')[0])\n",
        "\n",
        "train_features_tensor = torch.cat(train_features_list, dim=0)"
      ],
      "metadata": {
        "id": "yLQWjdj-qwvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964583ac-9374-4b2a-ff3e-30d5074742f5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Извлечение признаков для тренировочных данных: 100%|██████████| 7167/7167 [01:48<00:00, 65.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r1nBgLQ0zYPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка тестовых признаков и меток\n",
        "test_features_list = []\n",
        "test_labels = []\n",
        "for image_name in test_images:\n",
        "    image_path = os.path.join(images_path, image_name)\n",
        "    image = Image.open(image_path)\n",
        "    image = processor(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    test_features_list.append(image_features)\n",
        "    test_labels.append(image_name.split('_')[0])\n",
        "\n",
        "test_features_tensor = torch.cat(test_features_list, dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "Nj29Q-QewtXL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "        'epoch': 20,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss_img,\n",
        "        }, f\"/content/drive/MyDrive/Colab Notebooks/model_checkpoint_clip_new_1.pt\")"
      ],
      "metadata": {
        "id": "TWSJZR9ro7mz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для оценки точности\n",
        "def evaluate_accuracy(images, features_tensor, labels, model, preprocess, device, top_k_list):\n",
        "    correct_counts = {k: 0 for k in top_k_list}\n",
        "    total_count = len(test_images)\n",
        "\n",
        "    for image_name in tqdm(images, desc=\"Оценка точности\"):\n",
        "        query_cat_id = image_name.split('_')[0]\n",
        "        query_image_path = os.path.join(images_path, image_name)\n",
        "        query_image = preprocess(Image.open(query_image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            query_image_features = model.encode_image(query_image)\n",
        "            query_image_features /= query_image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Исключение текущего изображения из features_tensor\n",
        "        current_idx = images.index(image_name)\n",
        "        current_feature = features_tensor[current_idx]\n",
        "        other_features_tensor = torch.cat((features_tensor[:current_idx], features_tensor[current_idx + 1:]), dim=0)\n",
        "        other_labels = labels[:current_idx] + labels[current_idx + 1:]\n",
        "\n",
        "        similarities = (query_image_features @ other_features_tensor.T).cpu().numpy().flatten()\n",
        "        sorted_indices = similarities.argsort()[::-1]\n",
        "\n",
        "        for k in top_k_list:\n",
        "            top_k_labels = [other_labels[idx] for idx in sorted_indices[:k]]\n",
        "            if query_cat_id in top_k_labels:\n",
        "                correct_counts[k] += 1\n",
        "\n",
        "    accuracies = {k: correct_counts[k] / total_count for k in top_k_list}\n",
        "    return accuracies\n",
        "\n",
        "\n",
        "\n",
        "# Оценка точности\n",
        "top_k_list = [1, 5, 10, 20, 100, 1000, 4000, 8000]\n",
        "accuracies = evaluate_accuracy(test_images, test_features_tensor, test_labels, model, processor, device, top_k_list)\n",
        "\n",
        "print(\"Точность предсказаний:\")\n",
        "for k, accuracy in accuracies.items():\n",
        "    print(f\"Top-{k}: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fVwxj9Nqoic",
        "outputId": "37abe7ca-d0f2-432e-ca70-2998476e5f65"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Оценка точности: 100%|██████████| 1566/1566 [00:23<00:00, 65.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность предсказаний:\n",
            "Top-1: 0.3851\n",
            "Top-5: 0.5658\n",
            "Top-10: 0.6494\n",
            "Top-20: 0.7248\n",
            "Top-100: 0.8557\n",
            "Top-1000: 0.8985\n",
            "Top-4000: 0.9017\n",
            "Top-8000: 0.9017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CryqILbDru4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(images, features_tensor, labels, model, preprocess, device, top_k_list):\n",
        "    correct_counts = {k: 0 for k in top_k_list}\n",
        "    total_count = len(images)\n",
        "\n",
        "    for image_name in tqdm(images, desc=\"Оценка точности\"):\n",
        "        query_cat_id = image_name.split('_')[0]\n",
        "        query_image_path = os.path.join(images_path, image_name)\n",
        "        query_image = preprocess(Image.open(query_image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            query_image_features = model.encode_image(query_image)\n",
        "            query_image_features /= query_image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Исключение текущего изображения из features_tensor\n",
        "        current_idx = images.index(image_name)\n",
        "        current_feature = features_tensor[current_idx].to(device)\n",
        "        other_features_tensor = torch.cat((features_tensor[:current_idx], features_tensor[current_idx + 1:]), dim=0).to(device)\n",
        "        other_labels = labels[:current_idx] + labels[current_idx + 1:]\n",
        "\n",
        "        similarities = (query_image_features @ other_features_tensor.T).cpu().numpy().flatten()\n",
        "        sorted_indices = similarities.argsort()[::-1]\n",
        "\n",
        "        for k in top_k_list:\n",
        "            top_k_labels = [other_labels[idx] for idx in sorted_indices[:k]]\n",
        "            if query_cat_id in top_k_labels:\n",
        "                correct_counts[k] += 1\n",
        "\n",
        "    accuracies = {k: correct_counts[k] / total_count for k in top_k_list}\n",
        "    return accuracies\n",
        "\n",
        "# Подготовка тестовых признаков и меток\n",
        "def prepare_features_and_labels(images, model, preprocess, device):\n",
        "    features_list = []\n",
        "    labels = []\n",
        "\n",
        "    for image_name in tqdm(images, desc=\"Извлечение признаков\"):\n",
        "        image_path = os.path.join(images_path, image_name)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(image)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        features_list.append(image_features.cpu())\n",
        "        labels.append(image_name.split('_')[0])\n",
        "\n",
        "    features_tensor = torch.cat(features_list, dim=0)\n",
        "    return features_tensor, labels\n",
        "\n",
        "# Подготовка тренировочных признаков и меток\n",
        "train_features_tensor, train_labels = prepare_features_and_labels(train_images, model, processor, device)\n",
        "\n",
        "# Оценка точности на тренировочном наборе\n",
        "top_k_list = [1, 5, 10, 20, 100, 1000, 4000, 7167, 8000]\n",
        "train_accuracies = evaluate_accuracy(train_images, train_features_tensor, train_labels, model, processor, device, top_k_list)\n",
        "\n",
        "# Train images: 7167\n",
        "# Number of train cats: 2672\n",
        "\n",
        "print(\"Точность предсказаний на тренировочном наборе:\")\n",
        "for k, accuracy in train_accuracies.items():\n",
        "    print(f\"Top-{k}: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "5v8nFBk01tXD",
        "outputId": "3fa69425-0fc4-4c62-f191-4e8f4b59d5a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Извлечение признаков: 100%|██████████| 7167/7167 [01:43<00:00, 69.10it/s]\n",
            "Оценка точности: 100%|██████████| 7167/7167 [02:25<00:00, 49.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность предсказаний на тренировочном наборе:\n",
            "Top-1: 0.4710\n",
            "Top-5: 0.6734\n",
            "Top-10: 0.7449\n",
            "Top-20: 0.8001\n",
            "Top-100: 0.8776\n",
            "Top-1000: 0.8942\n",
            "Top-4000: 0.8961\n",
            "Top-7167: 0.8962\n",
            "Top-8000: 0.8962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test images: 1566\n",
        "# Number of test cats: 574\n",
        "\n",
        "top_k_list = [1, 5, 10, 20, 100, 1000, 1566, 1567]\n",
        "\n",
        "test_accuracies = evaluate_accuracy(test_images, test_features_tensor, test_labels, model, processor, device, top_k_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Точность предсказаний на теством наборе:\")\n",
        "for k, accuracy in test_accuracies.items():\n",
        "    print(f\"Top-{k}: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "ptNHl59K-kry",
        "outputId": "c1498a73-c15e-49ca-b8fe-2638fe71e00e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Оценка точности: 100%|██████████| 1566/1566 [00:24<00:00, 64.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность предсказаний на теством наборе:\n",
            "Top-1: 0.3851\n",
            "Top-5: 0.5658\n",
            "Top-10: 0.6494\n",
            "Top-20: 0.7248\n",
            "Top-100: 0.8557\n",
            "Top-1000: 0.8985\n",
            "Top-4000: 0.9017\n",
            "Top-7167: 0.9017\n",
            "Top-8000: 0.9017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipjNwXKz-3Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TvRZxPyJ-3G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/Colab Notebooks/model_checkpoint_clip_new_1.pt\")\n",
        "\n",
        "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"]\n",
        "checkpoint['model_state_dict'][\"input_resolution\"] = model_.input_resolution #default is 224\n",
        "checkpoint['model_state_dict'][\"context_length\"] = model_.context_length # default is 77\n",
        "checkpoint['model_state_dict'][\"vocab_size\"] = model_.vocab_size\n",
        "\n",
        "model_.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "zLk3yPLbpSwU",
        "outputId": "2b1a6cef-0cc7-44d9-818a-ecf20a03fd85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CLIP' object has no attribute 'input_resolution'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-e6527adb95f7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_resolution\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_resolution\u001b[0m \u001b[0;31m#default is 224\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_length\u001b[0m \u001b[0;31m# default is 77\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab_size\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CLIP' object has no attribute 'input_resolution'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Загрузка модели и процессора\n",
        "model_, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False) # Must set jit=False for training\n",
        "\n",
        "# Загрузка контрольной точки (checkpoint)\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/Colab Notebooks/model_checkpoint_clip_new_1.pt\", map_location=device)\n",
        "\n",
        "# Загрузка состояния модели\n",
        "model_.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(\"Модель успешно загружена\")"
      ],
      "metadata": {
        "id": "FLCQ0HMe1tZk",
        "outputId": "d08599c6-3ae5-4b72-ccf3-18a6f6b3e1cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель успешно загружена\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracies = evaluate_accuracy(test_images, test_features_tensor, test_labels, model_, preprocess, device, top_k_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7aOl1t7v1tcv",
        "outputId": "c17e6f7c-5114-49e7-af71-d2272d54f920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Оценка точности: 100%|██████████| 1566/1566 [00:24<00:00, 63.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка последних слоев для дообучения\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Настройка некоторых параметров для дообучения\n",
        "model.visual.proj.requires_grad = True\n",
        "model.transformer.resblocks[-1].mlp.c_fc.requires_grad = True\n",
        "model.transformer.resblocks[-1].mlp.c_proj.requires_grad = True\n",
        "\n",
        "# Определение функции потерь и оптимизатора\n",
        "loss_img = torch.nn.CrossEntropyLoss()\n",
        "loss_txt = torch.nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*epoch_num)\n",
        "\n",
        "# Функция для обучения модели\n",
        "def train_model_(train_loader, val_loader, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Эпоха {epoch + 1}/{num_epochs} - Обучение\")\n",
        "        for images, labels, details_text in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            details_text = details_text.squeeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits_per_image, logits_per_text = model(images, details_text)\n",
        "\n",
        "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "            total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "            total_loss.backward()\n",
        "            convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            clip.model.convert_weights(model)\n",
        "\n",
        "            running_loss += total_loss.item()\n",
        "            progress_bar.set_postfix(loss=running_loss / (progress_bar.n + 1))\n",
        "        print(f\"Эпоха {epoch + 1} завершена. Потеря на тренировочном наборе: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Оценка на валидационном наборе\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels, details_text in tqdm(val_loader, desc=f\"Эпоха {epoch + 1}/{num_epochs} - Валидация\"):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                details_text = details_text.squeeze(1).to(device)\n",
        "\n",
        "                logits_per_image, logits_per_text = model(images, details_text)\n",
        "\n",
        "                ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "                total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "\n",
        "                val_loss += total_loss.item()\n",
        "            print(f\"Потеря на валидационном наборе: {val_loss / len(val_loader)}\")\n",
        "\n",
        "# Обучение модели\n",
        "train_model_(train_loader, val_loader, optimizer, 20)\n",
        "\n",
        "# Сохранение модели\n",
        "torch.save(model.state_dict(), \"./parses/clip_finetuned.pth\")"
      ],
      "metadata": {
        "id": "jXpdaCC_YORD",
        "outputId": "9cd45ed5-4395-4c85-b8bf-aebaf9a054e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Эпоха 1/20 - Обучение:   0%|          | 0/112 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-9eda01ec9da6>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Обучение модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Сохранение модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-9eda01ec9da6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mconvert_models_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3427aa905e66>\u001b[0m in \u001b[0;36mconvert_models_to_fp32\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}